defaults:
  - _self_
  - encoder: identity
  - action_ae: discretizers/k_means_pusht_best
  - env: pusht 
  - state_prior: mingpt_pusht_best
  - env_vars: env_vars

lazy_init_models: True

# Dataset details
train_fraction: 0.95 # train-validation spit (used during training to avoid overfitting)
batch_size: 64
num_workers: 8
window_size: 14 # temporal window for delta_timestamps for lerobot, also that will be fed to the model

# Training details
num_training_epochs: 75
data_parallel: False
device: mps
optim: Adam
save_latents: False  # to save the latents after training the auto-encoder

lr: 1e-4
weight_decay: 0.1
betas:
  - 0.9
  - 0.95
grad_norm_clip: 1.0
seed: 42

# Prior training details
num_prior_epochs: 19 # training time epochs
eval_prior_every: 1 # for evaluation after every specified number of epochs
save_prior_every: 1 # for saving the checkpoints after specified num of epochs

# Logging frequency
eval_every: 1
save_every: 1

# Wandb config
project: behavior_transformer_repro_test
experiment: pusht_train

hydra:
  job:
    override_dirname: ${experiment}
  run:
    dir: ./exp_local/${now:%Y.%m.%d}/${now:%H%M%S}_${experiment}
  sweep:
    dir: ./exp/${now:%Y.%m.%d}/${now:%H%M}_${experiment}
    subdir: ${hydra.job.num}
